{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/game-addicted-man/2025DLP/blob/main/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%8B%A4%EC%8A%B5_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5evNZsjt2WF"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
        "/ [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials) / [Videos on YouTube](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)\n",
        "\n",
        "Modified by uramoon@kw.ac.kr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMtBnW6Vt2WJ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "감성 분석 (Sentiment Analysis)은 자연어 처리 (Natural Language Processing, NLP)를 이용하여 텍스트의 감정을 파악하는 기술이다. 이 노트북에서는 영화 리뷰가 긍정적인지 부정적인지 분류할 것이다.\n",
        "\n",
        "\"This movie is not very good.\"을 보면 \"very good\"은 긍정적인 감정이지만 \"not\"이 있기 때문에 부정적인 감정으로 분류되어야 한다. 이러한 것을 어떻게 학습시킬 수 있을까?\n",
        "\n",
        "1. 인공 신경망은 숫자를 입력으로 받아들이는데 텍스트를 어떻게 수치 데이터로 변환할지 생각해봐야 한다.\n",
        "2. 문서의 길이는 문서마다 다른데 크기가 각기 다른 입력 데이터를 인공 신경망에 어떻게 입력해야할지 생각해봐야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMoL_tZ7t2WJ"
      },
      "source": [
        "## Flowchart\n",
        "\n",
        "1. Tokenizer를 이용하여 각 단어를 정수로 변환한다. 예) the: 1, and: 2, a: 3, ...\n",
        "2. 임베딩 (embedding)을 통해 각 정수를 n차원 벡터로 변환한다. (가까운 단어는 가깝게 위치)\n",
        "3. 문서를 순환 신경망 (recurrent neural network, RNN)에 입력하여 0 (부정적)부터 1 (긍적적) 사이의 실수를 출력하게 훈련한다.\n",
        "\n",
        "The flowchart of the algorithm is roughly:\n",
        "\n",
        "<img src=\"https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_natural_language_flowchart.png?raw=1\" alt=\"Flowchart NLP\" style=\"width: 300px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdGt4vEDt2WK"
      },
      "source": [
        "## Recurrent Neural Network (RNN)\n",
        "\n",
        "RNN의 기본 유닛은 Recurrent Unit (RU)으로 LSTM (Long-Short-Term-Memory)과 성능 하락을 최소화하면서 LSTM을 단순화한 GRU (Gated Recurrent Unit)가 많이 사용된다.\n",
        "\n",
        "RU는 과거의 상태를 기억하고 있다가 현재의 입력에 대해 자신의 상태를 바꾸면서 값을 출력한다.\n",
        "\n",
        "새로운 상태는 과거의 상태와 현재의 입력에 따라 결정된다. 예를 들어 최근의 입력에 \"not\"이 있었고, 현재의 입력이 \"good\"이라면 새로운 상태는 \"not good\"에 해당하는 부정적인 감정을 기억할 것이다.\n",
        "\n",
        "![Recurrent unit](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_recurrent_unit.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTHceb8_t2WK"
      },
      "source": [
        "### Unrolled Network\n",
        "\n",
        "RNN에서 RU가 다음 입력에 과거의 정보를 전달하는 과정을 다음과 같이 펼쳐서 도식화할 수 있다.\n",
        "\n",
        "RU의 메모리는 0으로 초기화된다.\n",
        "\n",
        "가장 처음 \"this\"가 입력되면 메모리는 새로운 상태를 저장하고 무언가를 출력하지만 출력값을 사용하지는 않는다. 글을 끝까지 읽었을 때의 출력값만 활용할 것이다.\n",
        "\n",
        "두 번째 단어는 \"is\"인데 바로 전에 읽은 \"this\"와 결합하여 새로운 상태를 저장한다.\n",
        "\n",
        "세 번째 단어는 \"not\"인데 \"not\"을 기억하고 있다가 나중에 \"good\"을 읽었을 때 그것을 부정적인 단어로 바꿔주는 역할을 할 것이다.\n",
        "\n",
        "문서의 마지막 단어를 읽었을 때 0부터 1사이의 값을 출력하는데 이를 활용하여 감성 분석을 수행한다.\n",
        "\n",
        "Note that for the sake of clarity, this figure doesn't show the mapping from text-words to integer-tokens and embedding-vectors, as well as the fully-connected Sigmoid layer on the output.\n",
        "\n",
        "![Unrolled network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_unrolled_flowchart.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib5anMcIt2WL"
      },
      "source": [
        "### Exploding & Vanishing Gradients\n",
        "\n",
        "매 스텝 새로운 단어가 들어올 때마다 내부 상태를 변경할 경우 기울기 소실 혹은 폭주 문제가 발생할 수 있습니다.\n",
        "\n",
        "하나의 텍스트가 500개의 단어로 구성된다고 했을 때 내부 상태가 500번 변화하게 되는데 각 변화마다 기울기를 곱할 때 기울기가 1 미만이면 그 값이 0에 가까워지고, 기울기가 1보다 크면 그 값은 매우 커집니다.\n",
        "\n",
        "이러한 기울기 소실 / 폭주 문제를 해결하기 위해 매 입력마다 상태가 변화하지 않는 능력을 지닌 장기기억 메모리를 도입한 것이 LSTM과 GRU입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 준비 단계\n",
        "1. 런타임을 GPU로 설정해주세요.\n",
        "2. imdb.py와 download.py 파일을 Colab 환경에 복사해주세요."
      ],
      "metadata": {
        "id": "Pqee1-G7BMYi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwiNseb1t2WM"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NACTD0ZTt2WM"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsG8d_Kst2WN"
      },
      "source": [
        "We need to import several things from Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1NoU7t1Ht2WO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiZrp2Ppt2WP"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "IMDB에서 50,000개의 영화 리뷰를 다운받습니다. Keras에 정제된 IMDB 데이터셋이 있지만 단어를 정수로 변환하는 작업을 직접 수행하기 위해 정제되지 않은 데이터셋을 사용합니다. 50,000개 중 25,000개는 훈련 데이터, 나머지 25,000개는 테스트 데이터입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RZWc7HVCt2WP"
      },
      "outputs": [],
      "source": [
        "import imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfwqtMAtt2WQ"
      },
      "source": [
        "Automatically download and extract the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ShNhaNQ4t2WQ",
        "outputId": "a3a229f0-191d-458e-b2ef-cd0e52548bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "imdb.maybe_download_and_extract()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다운받은 파일 확인\n",
        "폴더를 새로고침하면 다운로드 받은 data 폴더가 보입니다.\n",
        "\n",
        "1. 훈련 데이터 폴더: data/IMDB/aclImdb/train/\n",
        "2. 테스트 데이터 폴더: data/IMDB/aclImdb/test/"
      ],
      "metadata": {
        "id": "rBXbQObrB5lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터의 pos 폴더에서 10점짜리 긍정적 리뷰 확인\n",
        "#  This movie gets better each time I see it (which is quite often).\n",
        "!cat data/IMDB/aclImdb/train/pos/10001_10.txt"
      ],
      "metadata": {
        "id": "KctXlcN9B4Nn",
        "outputId": "7d83a920-d727-480e-ca6e-181c00d2cc48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 데이터의 neg 폴더에서 1점짜리 부정적 리뷰 확인\n",
        "# this story is too painful to watch.\n",
        "!cat data/IMDB/aclImdb/train/neg/10002_1.txt"
      ],
      "metadata": {
        "id": "H0DdsDcDDDDO",
        "outputId": "4bbc8812-87b8-4693-ee4e-fa496c213309",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i'm no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI5HZG9ut2WQ"
      },
      "source": [
        "## 훈련 데이터와 테스트 데이터 만들기\n",
        "x 값에는 리뷰 내용이 (텍스트), y 값에는 0 (부정) 혹은 1 (긍정)이 기록됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "anIzfJ5-t2WQ"
      },
      "outputs": [],
      "source": [
        "x_train_text, y_train = imdb.load_data(train=True)\n",
        "x_test_text, y_test = imdb.load_data(train=False)\n",
        "\n",
        "# Convert to numpy arrays.\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N-fgoTa9t2WR",
        "outputId": "80d4084b-688e-4194-e59a-da0077dc144d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-set size:  25000\n",
            "Test-set size:   25000\n"
          ]
        }
      ],
      "source": [
        "print(\"Train-set size: \", len(x_train_text))\n",
        "print(\"Test-set size:  \", len(x_test_text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: 훈련데이터의 0번째 x값(리뷰)과 y값(정답)을 출력해보세요.\n",
        "print(x_train_text[0])\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "xI6MDlNIEVng",
        "outputId": "619ede0f-832a-42ea-a52d-2fcee2e37cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visually stunning and full of Eastern Philosophy, this amazing martial arts fantasy is brought to you by master director Tsui Hark, the man behind some of the best films Hong Kong cinema has produced. The special effects are beautiful and imaginative. The plot is a bit on the cerebral side, but is a refreshing change from films that treat their audience as if they were morons. If thinking is not your forte, however, this may not be your movie. Maybe you should go see the latest from the Hollywood studio's no brain club, but if you are looking for something more, he's where you will find it.\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAD8-TAWt2WS"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "인공신경망은 숫자를 입력받기 때문에 우선 각 단어를 tokenizer를 통해 정수로 변환합니다.<br>Tokenizer는 데이터셋에서 가장 많이 등장하는 n개의 단어를 정수로 변환합니다.<br> 예) the: 1, and: 2, a: 3, ...\n",
        "<br>\n",
        "일단 10000개의 단어만 사용하여 모델을 만들어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UZtN0bXxt2WS"
      },
      "outputs": [],
      "source": [
        "num_words = 10000\n",
        "tokenizer = Tokenizer(num_words=num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEZUMyLIt2WS"
      },
      "source": [
        "훈련 데이터와 테스트 데이터에 들어 있는 모든 단어를 변환할 필요가 있기 때문에 훈련 데이터와 테스트 데이터를 함께 tokenizer에 입력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5qXkL1ONt2WS",
        "outputId": "cdc9a669-5a5d-4876-e922-37d72cd61f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.67 s, sys: 43.1 ms, total: 7.72 s\n",
            "Wall time: 7.76 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "data_text = x_train_text + x_test_text\n",
        "tokenizer.fit_on_texts(data_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvJr9Xfyt2WT"
      },
      "source": [
        "Tokenizer가 찾아낸 많이 등장하는 단어들입니다. 해당 단어들은 정수로 변환되고 나머지 단어들은 제거될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZU5AyhO8t2WT",
        "outputId": "d0a01639-ead1-442b-f898-7ea603707bab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'a': 3,\n",
              " 'of': 4,\n",
              " 'to': 5,\n",
              " 'is': 6,\n",
              " 'br': 7,\n",
              " 'in': 8,\n",
              " 'it': 9,\n",
              " 'i': 10,\n",
              " 'this': 11,\n",
              " 'that': 12,\n",
              " 'was': 13,\n",
              " 'as': 14,\n",
              " 'for': 15,\n",
              " 'with': 16,\n",
              " 'movie': 17,\n",
              " 'but': 18,\n",
              " 'film': 19,\n",
              " 'on': 20,\n",
              " 'not': 21,\n",
              " 'you': 22,\n",
              " 'are': 23,\n",
              " 'his': 24,\n",
              " 'have': 25,\n",
              " 'be': 26,\n",
              " 'one': 27,\n",
              " 'he': 28,\n",
              " 'all': 29,\n",
              " 'at': 30,\n",
              " 'by': 31,\n",
              " 'an': 32,\n",
              " 'they': 33,\n",
              " 'so': 34,\n",
              " 'who': 35,\n",
              " 'from': 36,\n",
              " 'like': 37,\n",
              " 'or': 38,\n",
              " 'just': 39,\n",
              " 'her': 40,\n",
              " 'out': 41,\n",
              " 'about': 42,\n",
              " 'if': 43,\n",
              " \"it's\": 44,\n",
              " 'has': 45,\n",
              " 'there': 46,\n",
              " 'some': 47,\n",
              " 'what': 48,\n",
              " 'good': 49,\n",
              " 'when': 50,\n",
              " 'more': 51,\n",
              " 'very': 52,\n",
              " 'up': 53,\n",
              " 'no': 54,\n",
              " 'time': 55,\n",
              " 'my': 56,\n",
              " 'even': 57,\n",
              " 'would': 58,\n",
              " 'she': 59,\n",
              " 'which': 60,\n",
              " 'only': 61,\n",
              " 'really': 62,\n",
              " 'see': 63,\n",
              " 'story': 64,\n",
              " 'their': 65,\n",
              " 'had': 66,\n",
              " 'can': 67,\n",
              " 'me': 68,\n",
              " 'well': 69,\n",
              " 'were': 70,\n",
              " 'than': 71,\n",
              " 'much': 72,\n",
              " 'we': 73,\n",
              " 'bad': 74,\n",
              " 'been': 75,\n",
              " 'get': 76,\n",
              " 'do': 77,\n",
              " 'great': 78,\n",
              " 'other': 79,\n",
              " 'will': 80,\n",
              " 'also': 81,\n",
              " 'into': 82,\n",
              " 'people': 83,\n",
              " 'because': 84,\n",
              " 'how': 85,\n",
              " 'first': 86,\n",
              " 'him': 87,\n",
              " 'most': 88,\n",
              " \"don't\": 89,\n",
              " 'made': 90,\n",
              " 'then': 91,\n",
              " 'its': 92,\n",
              " 'them': 93,\n",
              " 'make': 94,\n",
              " 'way': 95,\n",
              " 'too': 96,\n",
              " 'movies': 97,\n",
              " 'could': 98,\n",
              " 'any': 99,\n",
              " 'after': 100,\n",
              " 'think': 101,\n",
              " 'characters': 102,\n",
              " 'watch': 103,\n",
              " 'films': 104,\n",
              " 'two': 105,\n",
              " 'many': 106,\n",
              " 'seen': 107,\n",
              " 'character': 108,\n",
              " 'being': 109,\n",
              " 'never': 110,\n",
              " 'plot': 111,\n",
              " 'love': 112,\n",
              " 'acting': 113,\n",
              " 'life': 114,\n",
              " 'did': 115,\n",
              " 'best': 116,\n",
              " 'where': 117,\n",
              " 'know': 118,\n",
              " 'show': 119,\n",
              " 'little': 120,\n",
              " 'over': 121,\n",
              " 'off': 122,\n",
              " 'ever': 123,\n",
              " 'does': 124,\n",
              " 'your': 125,\n",
              " 'better': 126,\n",
              " 'end': 127,\n",
              " 'man': 128,\n",
              " 'scene': 129,\n",
              " 'still': 130,\n",
              " 'say': 131,\n",
              " 'these': 132,\n",
              " 'here': 133,\n",
              " 'scenes': 134,\n",
              " 'why': 135,\n",
              " 'while': 136,\n",
              " 'something': 137,\n",
              " 'such': 138,\n",
              " 'go': 139,\n",
              " 'through': 140,\n",
              " 'back': 141,\n",
              " 'should': 142,\n",
              " 'those': 143,\n",
              " 'real': 144,\n",
              " \"i'm\": 145,\n",
              " 'now': 146,\n",
              " 'watching': 147,\n",
              " 'thing': 148,\n",
              " \"doesn't\": 149,\n",
              " 'actors': 150,\n",
              " 'though': 151,\n",
              " 'funny': 152,\n",
              " 'years': 153,\n",
              " \"didn't\": 154,\n",
              " 'old': 155,\n",
              " 'another': 156,\n",
              " '10': 157,\n",
              " 'work': 158,\n",
              " 'before': 159,\n",
              " 'actually': 160,\n",
              " 'nothing': 161,\n",
              " 'makes': 162,\n",
              " 'look': 163,\n",
              " 'director': 164,\n",
              " 'find': 165,\n",
              " 'going': 166,\n",
              " 'same': 167,\n",
              " 'new': 168,\n",
              " 'lot': 169,\n",
              " 'every': 170,\n",
              " 'few': 171,\n",
              " 'again': 172,\n",
              " 'part': 173,\n",
              " 'cast': 174,\n",
              " 'down': 175,\n",
              " 'us': 176,\n",
              " 'things': 177,\n",
              " 'want': 178,\n",
              " 'quite': 179,\n",
              " 'pretty': 180,\n",
              " 'world': 181,\n",
              " 'horror': 182,\n",
              " 'around': 183,\n",
              " 'seems': 184,\n",
              " \"can't\": 185,\n",
              " 'young': 186,\n",
              " 'take': 187,\n",
              " 'however': 188,\n",
              " 'got': 189,\n",
              " 'thought': 190,\n",
              " 'big': 191,\n",
              " 'fact': 192,\n",
              " 'enough': 193,\n",
              " 'long': 194,\n",
              " 'both': 195,\n",
              " \"that's\": 196,\n",
              " 'give': 197,\n",
              " \"i've\": 198,\n",
              " 'own': 199,\n",
              " 'may': 200,\n",
              " 'between': 201,\n",
              " 'comedy': 202,\n",
              " 'right': 203,\n",
              " 'series': 204,\n",
              " 'action': 205,\n",
              " 'must': 206,\n",
              " 'music': 207,\n",
              " 'without': 208,\n",
              " 'times': 209,\n",
              " 'saw': 210,\n",
              " 'always': 211,\n",
              " 'original': 212,\n",
              " \"isn't\": 213,\n",
              " 'role': 214,\n",
              " 'come': 215,\n",
              " 'almost': 216,\n",
              " 'gets': 217,\n",
              " 'interesting': 218,\n",
              " 'guy': 219,\n",
              " 'point': 220,\n",
              " 'done': 221,\n",
              " \"there's\": 222,\n",
              " 'whole': 223,\n",
              " 'least': 224,\n",
              " 'far': 225,\n",
              " 'bit': 226,\n",
              " 'script': 227,\n",
              " 'minutes': 228,\n",
              " 'feel': 229,\n",
              " '2': 230,\n",
              " 'anything': 231,\n",
              " 'making': 232,\n",
              " 'might': 233,\n",
              " 'since': 234,\n",
              " 'am': 235,\n",
              " 'family': 236,\n",
              " \"he's\": 237,\n",
              " 'last': 238,\n",
              " 'probably': 239,\n",
              " 'tv': 240,\n",
              " 'performance': 241,\n",
              " 'kind': 242,\n",
              " 'away': 243,\n",
              " 'yet': 244,\n",
              " 'fun': 245,\n",
              " 'worst': 246,\n",
              " 'sure': 247,\n",
              " 'rather': 248,\n",
              " 'hard': 249,\n",
              " 'anyone': 250,\n",
              " 'girl': 251,\n",
              " 'each': 252,\n",
              " 'played': 253,\n",
              " 'day': 254,\n",
              " 'found': 255,\n",
              " 'looking': 256,\n",
              " 'woman': 257,\n",
              " 'screen': 258,\n",
              " 'although': 259,\n",
              " 'our': 260,\n",
              " 'especially': 261,\n",
              " 'believe': 262,\n",
              " 'having': 263,\n",
              " 'trying': 264,\n",
              " 'course': 265,\n",
              " 'dvd': 266,\n",
              " 'everything': 267,\n",
              " 'set': 268,\n",
              " 'goes': 269,\n",
              " 'comes': 270,\n",
              " 'put': 271,\n",
              " 'ending': 272,\n",
              " 'maybe': 273,\n",
              " 'place': 274,\n",
              " 'book': 275,\n",
              " 'shows': 276,\n",
              " 'three': 277,\n",
              " 'worth': 278,\n",
              " 'different': 279,\n",
              " 'main': 280,\n",
              " 'once': 281,\n",
              " 'sense': 282,\n",
              " 'american': 283,\n",
              " 'reason': 284,\n",
              " 'looks': 285,\n",
              " 'effects': 286,\n",
              " 'watched': 287,\n",
              " 'play': 288,\n",
              " 'true': 289,\n",
              " 'money': 290,\n",
              " 'actor': 291,\n",
              " \"wasn't\": 292,\n",
              " 'job': 293,\n",
              " 'together': 294,\n",
              " 'war': 295,\n",
              " 'someone': 296,\n",
              " 'plays': 297,\n",
              " 'instead': 298,\n",
              " 'high': 299,\n",
              " 'during': 300,\n",
              " 'year': 301,\n",
              " 'said': 302,\n",
              " 'half': 303,\n",
              " 'everyone': 304,\n",
              " 'later': 305,\n",
              " 'takes': 306,\n",
              " '1': 307,\n",
              " 'seem': 308,\n",
              " 'audience': 309,\n",
              " 'special': 310,\n",
              " 'beautiful': 311,\n",
              " 'left': 312,\n",
              " 'himself': 313,\n",
              " 'seeing': 314,\n",
              " 'john': 315,\n",
              " 'night': 316,\n",
              " 'black': 317,\n",
              " 'version': 318,\n",
              " 'shot': 319,\n",
              " 'excellent': 320,\n",
              " 'idea': 321,\n",
              " 'house': 322,\n",
              " 'mind': 323,\n",
              " 'star': 324,\n",
              " 'wife': 325,\n",
              " 'fan': 326,\n",
              " 'death': 327,\n",
              " 'used': 328,\n",
              " 'else': 329,\n",
              " 'simply': 330,\n",
              " 'nice': 331,\n",
              " 'budget': 332,\n",
              " 'poor': 333,\n",
              " 'short': 334,\n",
              " 'completely': 335,\n",
              " 'second': 336,\n",
              " \"you're\": 337,\n",
              " '3': 338,\n",
              " 'read': 339,\n",
              " 'less': 340,\n",
              " 'along': 341,\n",
              " 'top': 342,\n",
              " 'help': 343,\n",
              " 'home': 344,\n",
              " 'men': 345,\n",
              " 'either': 346,\n",
              " 'line': 347,\n",
              " 'boring': 348,\n",
              " 'dead': 349,\n",
              " 'friends': 350,\n",
              " 'kids': 351,\n",
              " 'try': 352,\n",
              " 'production': 353,\n",
              " 'enjoy': 354,\n",
              " 'camera': 355,\n",
              " 'use': 356,\n",
              " 'wrong': 357,\n",
              " 'given': 358,\n",
              " 'low': 359,\n",
              " 'classic': 360,\n",
              " 'father': 361,\n",
              " 'need': 362,\n",
              " 'full': 363,\n",
              " 'stupid': 364,\n",
              " 'next': 365,\n",
              " 'until': 366,\n",
              " 'performances': 367,\n",
              " 'school': 368,\n",
              " 'hollywood': 369,\n",
              " 'rest': 370,\n",
              " 'truly': 371,\n",
              " 'awful': 372,\n",
              " 'video': 373,\n",
              " 'couple': 374,\n",
              " 'start': 375,\n",
              " 'sex': 376,\n",
              " 'recommend': 377,\n",
              " 'women': 378,\n",
              " 'let': 379,\n",
              " 'tell': 380,\n",
              " 'terrible': 381,\n",
              " 'remember': 382,\n",
              " 'mean': 383,\n",
              " 'came': 384,\n",
              " 'understand': 385,\n",
              " 'getting': 386,\n",
              " 'perhaps': 387,\n",
              " 'moments': 388,\n",
              " 'name': 389,\n",
              " 'keep': 390,\n",
              " 'face': 391,\n",
              " 'itself': 392,\n",
              " 'wonderful': 393,\n",
              " 'playing': 394,\n",
              " 'human': 395,\n",
              " 'style': 396,\n",
              " 'small': 397,\n",
              " 'episode': 398,\n",
              " 'perfect': 399,\n",
              " 'others': 400,\n",
              " 'person': 401,\n",
              " 'doing': 402,\n",
              " 'often': 403,\n",
              " 'early': 404,\n",
              " 'stars': 405,\n",
              " 'definitely': 406,\n",
              " 'written': 407,\n",
              " 'head': 408,\n",
              " 'lines': 409,\n",
              " 'dialogue': 410,\n",
              " 'gives': 411,\n",
              " 'piece': 412,\n",
              " \"couldn't\": 413,\n",
              " 'went': 414,\n",
              " 'finally': 415,\n",
              " 'mother': 416,\n",
              " 'title': 417,\n",
              " 'case': 418,\n",
              " 'absolutely': 419,\n",
              " 'boy': 420,\n",
              " 'live': 421,\n",
              " 'yes': 422,\n",
              " 'laugh': 423,\n",
              " 'certainly': 424,\n",
              " 'liked': 425,\n",
              " 'become': 426,\n",
              " 'entertaining': 427,\n",
              " 'worse': 428,\n",
              " 'oh': 429,\n",
              " 'sort': 430,\n",
              " 'loved': 431,\n",
              " 'lost': 432,\n",
              " 'called': 433,\n",
              " 'hope': 434,\n",
              " 'picture': 435,\n",
              " 'felt': 436,\n",
              " 'overall': 437,\n",
              " 'entire': 438,\n",
              " 'several': 439,\n",
              " 'mr': 440,\n",
              " 'based': 441,\n",
              " 'supposed': 442,\n",
              " 'cinema': 443,\n",
              " 'friend': 444,\n",
              " 'guys': 445,\n",
              " 'sound': 446,\n",
              " '5': 447,\n",
              " 'problem': 448,\n",
              " 'drama': 449,\n",
              " 'against': 450,\n",
              " 'waste': 451,\n",
              " 'white': 452,\n",
              " 'beginning': 453,\n",
              " '4': 454,\n",
              " 'fans': 455,\n",
              " 'totally': 456,\n",
              " 'dark': 457,\n",
              " 'care': 458,\n",
              " 'direction': 459,\n",
              " 'humor': 460,\n",
              " 'wanted': 461,\n",
              " \"she's\": 462,\n",
              " 'seemed': 463,\n",
              " 'game': 464,\n",
              " 'under': 465,\n",
              " 'children': 466,\n",
              " 'despite': 467,\n",
              " 'lives': 468,\n",
              " 'lead': 469,\n",
              " 'guess': 470,\n",
              " 'example': 471,\n",
              " 'already': 472,\n",
              " 'final': 473,\n",
              " 'throughout': 474,\n",
              " \"you'll\": 475,\n",
              " 'evil': 476,\n",
              " 'turn': 477,\n",
              " 'becomes': 478,\n",
              " 'unfortunately': 479,\n",
              " 'able': 480,\n",
              " 'quality': 481,\n",
              " \"i'd\": 482,\n",
              " 'days': 483,\n",
              " 'history': 484,\n",
              " 'fine': 485,\n",
              " 'side': 486,\n",
              " 'wants': 487,\n",
              " 'horrible': 488,\n",
              " 'heart': 489,\n",
              " 'writing': 490,\n",
              " 'amazing': 491,\n",
              " 'b': 492,\n",
              " 'flick': 493,\n",
              " 'killer': 494,\n",
              " 'run': 495,\n",
              " 'son': 496,\n",
              " '\\x96': 497,\n",
              " 'michael': 498,\n",
              " 'works': 499,\n",
              " 'close': 500,\n",
              " \"they're\": 501,\n",
              " 'act': 502,\n",
              " 'art': 503,\n",
              " 'kill': 504,\n",
              " 'matter': 505,\n",
              " 'etc': 506,\n",
              " 'tries': 507,\n",
              " \"won't\": 508,\n",
              " 'past': 509,\n",
              " 'town': 510,\n",
              " 'enjoyed': 511,\n",
              " 'turns': 512,\n",
              " 'brilliant': 513,\n",
              " 'gave': 514,\n",
              " 'behind': 515,\n",
              " 'parts': 516,\n",
              " 'stuff': 517,\n",
              " 'genre': 518,\n",
              " 'eyes': 519,\n",
              " 'car': 520,\n",
              " 'favorite': 521,\n",
              " 'directed': 522,\n",
              " 'late': 523,\n",
              " 'hand': 524,\n",
              " 'expect': 525,\n",
              " 'soon': 526,\n",
              " 'hour': 527,\n",
              " 'obviously': 528,\n",
              " 'themselves': 529,\n",
              " 'sometimes': 530,\n",
              " 'killed': 531,\n",
              " 'thinking': 532,\n",
              " 'actress': 533,\n",
              " 'girls': 534,\n",
              " 'child': 535,\n",
              " 'viewer': 536,\n",
              " 'starts': 537,\n",
              " 'city': 538,\n",
              " 'myself': 539,\n",
              " 'decent': 540,\n",
              " 'highly': 541,\n",
              " 'stop': 542,\n",
              " 'type': 543,\n",
              " 'self': 544,\n",
              " 'god': 545,\n",
              " 'says': 546,\n",
              " 'group': 547,\n",
              " 'anyway': 548,\n",
              " 'voice': 549,\n",
              " 'took': 550,\n",
              " 'known': 551,\n",
              " 'blood': 552,\n",
              " 'kid': 553,\n",
              " 'heard': 554,\n",
              " 'happens': 555,\n",
              " 'except': 556,\n",
              " 'fight': 557,\n",
              " 'feeling': 558,\n",
              " 'experience': 559,\n",
              " 'coming': 560,\n",
              " 'slow': 561,\n",
              " 'daughter': 562,\n",
              " 'writer': 563,\n",
              " 'stories': 564,\n",
              " 'moment': 565,\n",
              " 'leave': 566,\n",
              " 'told': 567,\n",
              " 'extremely': 568,\n",
              " 'score': 569,\n",
              " 'violence': 570,\n",
              " 'police': 571,\n",
              " 'involved': 572,\n",
              " 'strong': 573,\n",
              " 'chance': 574,\n",
              " 'lack': 575,\n",
              " 'cannot': 576,\n",
              " 'hit': 577,\n",
              " 'roles': 578,\n",
              " 'hilarious': 579,\n",
              " 's': 580,\n",
              " 'wonder': 581,\n",
              " 'happen': 582,\n",
              " 'particularly': 583,\n",
              " 'ok': 584,\n",
              " 'including': 585,\n",
              " 'living': 586,\n",
              " 'save': 587,\n",
              " 'looked': 588,\n",
              " \"wouldn't\": 589,\n",
              " 'crap': 590,\n",
              " 'simple': 591,\n",
              " 'please': 592,\n",
              " 'cool': 593,\n",
              " 'murder': 594,\n",
              " 'obvious': 595,\n",
              " 'happened': 596,\n",
              " 'complete': 597,\n",
              " 'cut': 598,\n",
              " 'age': 599,\n",
              " 'serious': 600,\n",
              " 'gore': 601,\n",
              " 'attempt': 602,\n",
              " 'hell': 603,\n",
              " 'ago': 604,\n",
              " 'song': 605,\n",
              " 'shown': 606,\n",
              " 'taken': 607,\n",
              " 'english': 608,\n",
              " 'james': 609,\n",
              " 'robert': 610,\n",
              " 'david': 611,\n",
              " 'seriously': 612,\n",
              " 'released': 613,\n",
              " 'reality': 614,\n",
              " 'opening': 615,\n",
              " 'interest': 616,\n",
              " 'jokes': 617,\n",
              " 'across': 618,\n",
              " 'none': 619,\n",
              " 'hero': 620,\n",
              " 'today': 621,\n",
              " 'exactly': 622,\n",
              " 'possible': 623,\n",
              " 'alone': 624,\n",
              " 'sad': 625,\n",
              " 'brother': 626,\n",
              " 'number': 627,\n",
              " 'career': 628,\n",
              " 'saying': 629,\n",
              " \"film's\": 630,\n",
              " 'hours': 631,\n",
              " 'usually': 632,\n",
              " 'cinematography': 633,\n",
              " 'talent': 634,\n",
              " 'view': 635,\n",
              " 'yourself': 636,\n",
              " 'running': 637,\n",
              " 'annoying': 638,\n",
              " 'relationship': 639,\n",
              " 'documentary': 640,\n",
              " 'wish': 641,\n",
              " 'order': 642,\n",
              " 'huge': 643,\n",
              " 'whose': 644,\n",
              " 'shots': 645,\n",
              " 'ridiculous': 646,\n",
              " 'taking': 647,\n",
              " 'important': 648,\n",
              " 'light': 649,\n",
              " 'body': 650,\n",
              " 'middle': 651,\n",
              " 'level': 652,\n",
              " 'ends': 653,\n",
              " 'started': 654,\n",
              " 'female': 655,\n",
              " 'call': 656,\n",
              " \"i'll\": 657,\n",
              " 'husband': 658,\n",
              " 'four': 659,\n",
              " 'power': 660,\n",
              " 'major': 661,\n",
              " 'turned': 662,\n",
              " 'word': 663,\n",
              " 'opinion': 664,\n",
              " 'change': 665,\n",
              " 'mostly': 666,\n",
              " 'usual': 667,\n",
              " 'silly': 668,\n",
              " 'scary': 669,\n",
              " 'rating': 670,\n",
              " 'beyond': 671,\n",
              " 'somewhat': 672,\n",
              " 'happy': 673,\n",
              " 'ones': 674,\n",
              " 'words': 675,\n",
              " 'room': 676,\n",
              " 'knew': 677,\n",
              " 'knows': 678,\n",
              " 'country': 679,\n",
              " 'disappointed': 680,\n",
              " 'talking': 681,\n",
              " 'novel': 682,\n",
              " 'apparently': 683,\n",
              " 'non': 684,\n",
              " 'strange': 685,\n",
              " 'upon': 686,\n",
              " 'attention': 687,\n",
              " 'finds': 688,\n",
              " 'single': 689,\n",
              " 'basically': 690,\n",
              " 'cheap': 691,\n",
              " 'modern': 692,\n",
              " 'due': 693,\n",
              " 'jack': 694,\n",
              " 'musical': 695,\n",
              " 'television': 696,\n",
              " 'problems': 697,\n",
              " 'miss': 698,\n",
              " 'episodes': 699,\n",
              " 'clearly': 700,\n",
              " 'local': 701,\n",
              " '7': 702,\n",
              " 'british': 703,\n",
              " 'thriller': 704,\n",
              " 'talk': 705,\n",
              " 'events': 706,\n",
              " 'sequence': 707,\n",
              " 'five': 708,\n",
              " \"aren't\": 709,\n",
              " 'class': 710,\n",
              " 'french': 711,\n",
              " 'moving': 712,\n",
              " 'ten': 713,\n",
              " 'fast': 714,\n",
              " 'earth': 715,\n",
              " 'review': 716,\n",
              " 'tells': 717,\n",
              " 'predictable': 718,\n",
              " 'songs': 719,\n",
              " 'team': 720,\n",
              " 'comic': 721,\n",
              " 'straight': 722,\n",
              " 'whether': 723,\n",
              " '8': 724,\n",
              " 'die': 725,\n",
              " 'add': 726,\n",
              " 'dialog': 727,\n",
              " 'entertainment': 728,\n",
              " 'above': 729,\n",
              " 'sets': 730,\n",
              " 'future': 731,\n",
              " 'enjoyable': 732,\n",
              " 'appears': 733,\n",
              " 'near': 734,\n",
              " 'space': 735,\n",
              " 'easily': 736,\n",
              " 'hate': 737,\n",
              " 'soundtrack': 738,\n",
              " 'bring': 739,\n",
              " 'giving': 740,\n",
              " 'lots': 741,\n",
              " 'similar': 742,\n",
              " 'romantic': 743,\n",
              " 'george': 744,\n",
              " 'supporting': 745,\n",
              " 'release': 746,\n",
              " 'mention': 747,\n",
              " 'within': 748,\n",
              " 'filmed': 749,\n",
              " 'message': 750,\n",
              " 'sequel': 751,\n",
              " 'clear': 752,\n",
              " 'falls': 753,\n",
              " 'needs': 754,\n",
              " \"haven't\": 755,\n",
              " 'dull': 756,\n",
              " 'suspense': 757,\n",
              " 'eye': 758,\n",
              " 'bunch': 759,\n",
              " 'surprised': 760,\n",
              " 'showing': 761,\n",
              " 'tried': 762,\n",
              " 'sorry': 763,\n",
              " 'certain': 764,\n",
              " 'easy': 765,\n",
              " 'working': 766,\n",
              " 'ways': 767,\n",
              " 'theme': 768,\n",
              " 'theater': 769,\n",
              " 'among': 770,\n",
              " 'named': 771,\n",
              " \"what's\": 772,\n",
              " 'storyline': 773,\n",
              " 'monster': 774,\n",
              " 'king': 775,\n",
              " 'stay': 776,\n",
              " 'effort': 777,\n",
              " 'stand': 778,\n",
              " 'minute': 779,\n",
              " 'fall': 780,\n",
              " 'gone': 781,\n",
              " 'rock': 782,\n",
              " 'using': 783,\n",
              " '9': 784,\n",
              " 'feature': 785,\n",
              " 'comments': 786,\n",
              " 'buy': 787,\n",
              " \"'\": 788,\n",
              " 'typical': 789,\n",
              " 't': 790,\n",
              " 'sister': 791,\n",
              " 'editing': 792,\n",
              " 'tale': 793,\n",
              " 'avoid': 794,\n",
              " 'dr': 795,\n",
              " 'mystery': 796,\n",
              " 'deal': 797,\n",
              " 'doubt': 798,\n",
              " 'fantastic': 799,\n",
              " 'nearly': 800,\n",
              " 'kept': 801,\n",
              " 'feels': 802,\n",
              " 'okay': 803,\n",
              " 'subject': 804,\n",
              " 'viewing': 805,\n",
              " 'elements': 806,\n",
              " 'oscar': 807,\n",
              " 'check': 808,\n",
              " 'realistic': 809,\n",
              " 'points': 810,\n",
              " 'means': 811,\n",
              " 'greatest': 812,\n",
              " 'herself': 813,\n",
              " 'parents': 814,\n",
              " 'famous': 815,\n",
              " 'imagine': 816,\n",
              " 'rent': 817,\n",
              " 'viewers': 818,\n",
              " 'crime': 819,\n",
              " 'richard': 820,\n",
              " 'form': 821,\n",
              " 'peter': 822,\n",
              " 'actual': 823,\n",
              " 'lady': 824,\n",
              " 'general': 825,\n",
              " 'dog': 826,\n",
              " 'follow': 827,\n",
              " 'believable': 828,\n",
              " 'period': 829,\n",
              " 'red': 830,\n",
              " 'brought': 831,\n",
              " 'move': 832,\n",
              " 'material': 833,\n",
              " 'forget': 834,\n",
              " 'somehow': 835,\n",
              " 'begins': 836,\n",
              " 're': 837,\n",
              " 'reviews': 838,\n",
              " 'animation': 839,\n",
              " 'paul': 840,\n",
              " \"you've\": 841,\n",
              " 'leads': 842,\n",
              " 'weak': 843,\n",
              " 'figure': 844,\n",
              " 'surprise': 845,\n",
              " 'hear': 846,\n",
              " 'sit': 847,\n",
              " 'average': 848,\n",
              " 'open': 849,\n",
              " 'sequences': 850,\n",
              " 'killing': 851,\n",
              " 'atmosphere': 852,\n",
              " 'eventually': 853,\n",
              " 'tom': 854,\n",
              " 'learn': 855,\n",
              " 'premise': 856,\n",
              " 'wait': 857,\n",
              " '20': 858,\n",
              " 'sci': 859,\n",
              " 'deep': 860,\n",
              " 'fi': 861,\n",
              " 'expected': 862,\n",
              " 'whatever': 863,\n",
              " 'indeed': 864,\n",
              " 'particular': 865,\n",
              " 'note': 866,\n",
              " 'poorly': 867,\n",
              " 'lame': 868,\n",
              " 'dance': 869,\n",
              " 'imdb': 870,\n",
              " 'situation': 871,\n",
              " 'shame': 872,\n",
              " 'third': 873,\n",
              " 'box': 874,\n",
              " 'york': 875,\n",
              " 'truth': 876,\n",
              " 'decided': 877,\n",
              " 'free': 878,\n",
              " 'hot': 879,\n",
              " \"who's\": 880,\n",
              " 'difficult': 881,\n",
              " 'needed': 882,\n",
              " 'season': 883,\n",
              " 'acted': 884,\n",
              " 'leaves': 885,\n",
              " 'unless': 886,\n",
              " 'emotional': 887,\n",
              " 'romance': 888,\n",
              " 'possibly': 889,\n",
              " 'sexual': 890,\n",
              " 'gay': 891,\n",
              " 'boys': 892,\n",
              " 'footage': 893,\n",
              " 'write': 894,\n",
              " 'western': 895,\n",
              " 'forced': 896,\n",
              " 'credits': 897,\n",
              " 'became': 898,\n",
              " 'memorable': 899,\n",
              " 'doctor': 900,\n",
              " 'reading': 901,\n",
              " 'otherwise': 902,\n",
              " 'crew': 903,\n",
              " 'de': 904,\n",
              " 'begin': 905,\n",
              " 'air': 906,\n",
              " 'question': 907,\n",
              " 'society': 908,\n",
              " 'meet': 909,\n",
              " 'male': 910,\n",
              " \"let's\": 911,\n",
              " 'meets': 912,\n",
              " 'plus': 913,\n",
              " 'cheesy': 914,\n",
              " 'hands': 915,\n",
              " 'superb': 916,\n",
              " 'screenplay': 917,\n",
              " 'beauty': 918,\n",
              " 'interested': 919,\n",
              " 'street': 920,\n",
              " 'features': 921,\n",
              " 'masterpiece': 922,\n",
              " 'perfectly': 923,\n",
              " 'whom': 924,\n",
              " 'laughs': 925,\n",
              " 'stage': 926,\n",
              " 'nature': 927,\n",
              " 'effect': 928,\n",
              " 'comment': 929,\n",
              " 'forward': 930,\n",
              " 'nor': 931,\n",
              " 'e': 932,\n",
              " 'previous': 933,\n",
              " 'sounds': 934,\n",
              " 'badly': 935,\n",
              " 'japanese': 936,\n",
              " 'weird': 937,\n",
              " 'island': 938,\n",
              " 'personal': 939,\n",
              " 'inside': 940,\n",
              " 'quickly': 941,\n",
              " 'total': 942,\n",
              " 'keeps': 943,\n",
              " 'towards': 944,\n",
              " 'america': 945,\n",
              " 'result': 946,\n",
              " 'crazy': 947,\n",
              " 'battle': 948,\n",
              " 'worked': 949,\n",
              " 'incredibly': 950,\n",
              " 'setting': 951,\n",
              " 'earlier': 952,\n",
              " 'background': 953,\n",
              " 'mess': 954,\n",
              " 'cop': 955,\n",
              " 'writers': 956,\n",
              " 'fire': 957,\n",
              " 'copy': 958,\n",
              " 'realize': 959,\n",
              " 'unique': 960,\n",
              " 'dumb': 961,\n",
              " 'powerful': 962,\n",
              " 'lee': 963,\n",
              " 'mark': 964,\n",
              " 'business': 965,\n",
              " 'rate': 966,\n",
              " 'older': 967,\n",
              " 'dramatic': 968,\n",
              " 'pay': 969,\n",
              " 'following': 970,\n",
              " 'directors': 971,\n",
              " 'joke': 972,\n",
              " 'girlfriend': 973,\n",
              " 'plenty': 974,\n",
              " 'directing': 975,\n",
              " 'various': 976,\n",
              " 'baby': 977,\n",
              " 'creepy': 978,\n",
              " 'appear': 979,\n",
              " 'development': 980,\n",
              " 'brings': 981,\n",
              " 'front': 982,\n",
              " 'ask': 983,\n",
              " 'dream': 984,\n",
              " 'water': 985,\n",
              " 'admit': 986,\n",
              " 'rich': 987,\n",
              " 'bill': 988,\n",
              " 'apart': 989,\n",
              " 'joe': 990,\n",
              " 'political': 991,\n",
              " 'fairly': 992,\n",
              " 'leading': 993,\n",
              " 'reasons': 994,\n",
              " 'portrayed': 995,\n",
              " 'spent': 996,\n",
              " 'telling': 997,\n",
              " 'cover': 998,\n",
              " 'outside': 999,\n",
              " 'present': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZGGeitPt2WT"
      },
      "source": [
        "Tokenizer로 훈련 데이터의 텍스트를 정수로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sOOcMmI3t2WT"
      },
      "outputs": [],
      "source": [
        "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSI5KoKwt2WT"
      },
      "source": [
        "For example, here is a text from the training-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bD7v-6HVt2WU",
        "outputId": "614fa82c-71d1-4024-ff44-eb0a1e18fde5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visually stunning and full of Eastern Philosophy, this amazing martial arts fantasy is brought to you by master director Tsui Hark, the man behind some of the best films Hong Kong cinema has produced. The special effects are beautiful and imaginative. The plot is a bit on the cerebral side, but is a refreshing change from films that treat their audience as if they were morons. If thinking is not your forte, however, this may not be your movie. Maybe you should go see the latest from the Hollywood studio's no brain club, but if you are looking for something more, he's where you will find it.\n"
          ]
        }
      ],
      "source": [
        "# TODO: 훈련 데이터의 0번째 원본 텍스트(x_train_text) 출력해보기\n",
        "print(x_train_text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX8tA1JQt2WU"
      },
      "source": [
        "This text corresponds to the following list of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xVyrza1ct2WU",
        "outputId": "af705c82-7c45-42df-dbcb-a871af49a48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩된 텍스트 (x_train_tokens[0]): [2141, 1427, 2, 363, 4, 4161, 3922, 11, 491, 1694, 1703, 1029, 6, 831, 5, 22, 31, 1199, 164, 1, 128, 515, 47, 4, 1, 116, 104, 2860, 2249, 443, 45, 1033, 1, 310, 286, 23, 311, 2, 3233, 1, 111, 6, 3, 226, 20, 1, 6856, 486, 18, 6, 3, 2523, 665, 36, 104, 12, 1774, 65, 309, 14, 43, 33, 70, 6471, 43, 532, 6, 21, 125, 188, 11, 200, 21, 26, 125, 17, 273, 22, 142, 139, 63, 1, 2411, 36, 1, 369, 54, 1126, 1393, 18, 43, 22, 23, 256, 15, 137, 51, 237, 117, 22, 80, 165, 9]\n",
            "'first'  86\n",
            "'of'  4\n",
            "'all'  29\n"
          ]
        }
      ],
      "source": [
        "# TODO: 위에서 정수로 변환한 0번째 텍스트 출력해보기\n",
        "# first는 86, of는 4, all은 29로 변환됐는지 살펴보세요.\n",
        "print(\"정수 인코딩된 텍스트 (x_train_tokens[0]):\", x_train_tokens[0])\n",
        "print(\"'first' \", tokenizer.word_index.get(\"first\"))\n",
        "print(\"'of' \", tokenizer.word_index.get(\"of\"))\n",
        "print(\"'all' \", tokenizer.word_index.get(\"all\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvOL1jHAt2WU"
      },
      "source": [
        "We also need to convert the texts in the test-set to tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FAjPUecQt2WU"
      },
      "outputs": [],
      "source": [
        "# TODO: tokenizer로 전체 테스트 데이터를 정수로 변환해보세요.\n",
        "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHGyZPoEt2WU"
      },
      "source": [
        "## Padding and Truncating Data\n",
        "\n",
        "순환신경망은 임의의 길이를 지닌 입력을 처리할 수 있지만 훈련의 속도를 높이기 위해 동일 길이를 지닌 입력 데이터의 묶음 (batch) 단위로 훈련이 이루어지는 관계로 편의상 모든 입력 데이터의 길이를 동일하게 합니다. 모든 입력 데이터의 길이를 동일하게 하는 방법은 크게 두 가지가 있습니다.\n",
        "\n",
        "1. 가장 길이가 긴 리뷰의 길이에 맞추어 다른 리뷰들의 앞, 혹은 뒤에 공백을 삽입한다.\n",
        "\n",
        "2. 적당한 길이를 정한다음 길이가 긴 리뷰는 앞, 혹은 뒤를 잘라내고 짧은 리뷰는 앞, 혹은 뒤에 공백을 삽입한다.\n",
        "\n",
        "첫 번째 방법은 지나치게 긴 리뷰가 있을 경우 다른 리뷰들에 지나치게 많은 공백이 들어가 데이터셋이 커져 훈련하는데 오래 걸리고 메모리의 낭비도 심해지는 단점이 있어 우리는 두 번째 방법을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RlDFg_w0t2WU"
      },
      "outputs": [],
      "source": [
        "# 각 리뷰에 단어 (토큰)가 몇 개씩 들어있는지 세어봅시다.\n",
        "\n",
        "num_tokens = [] # 빈 리스트 생성\n",
        "\n",
        "# TODO: num_tokens에 각 리뷰의 단어 숫자를 저장하세요.\n",
        "# num_tokens 원소는 50,000개이며 처음 25,000은 훈련 데이터의 단어숫자가 저장되어 있고,\n",
        "# 나머지 25,000은 테스트 데이터의 단어숫자가 저장되어 있습니다.\n",
        "# Hint: x_train_tokens[0]은 첫 번째 훈련 데이터의 단어들이 저장되어 있습니다. 그 길이는 len 함수로 알 수 있습니다.\n",
        "\n",
        "num_tokens = [len(tokens) for tokens in x_train_tokens + tokenizer.texts_to_sequences(x_test_text)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: num_tokens 리스트를 num_tokens NumPy 배열로 변환하세요.\n",
        "num_tokens = np.array(num_tokens)"
      ],
      "metadata": {
        "id": "9kwihq8yLf6H"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYdxLK99t2WU"
      },
      "source": [
        "The average number of tokens in a sequence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "S0kuTohct2WV",
        "outputId": "87fd375c-b759-43aa-d0db-a1886dec0779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(221.27716)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# 모든 리뷰에서 사용한 평균 단어의 수는 다음과 같습니다.\n",
        "np.mean(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDu36sCDt2WV"
      },
      "source": [
        "The maximum number of tokens in a sequence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IKujyc3It2WV",
        "outputId": "a90568d0-3cdf-4e96-9f0a-0a80346eae0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가장 긴 리뷰의 단어 수: 2209\n"
          ]
        }
      ],
      "source": [
        "# TODO: 가장 긴 리뷰의 단어수를 출력해보세요.\n",
        "# Hint: np.max\n",
        "print(\"가장 긴 리뷰의 단어 수:\", np.max(num_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 리뷰에 사용된 단어수(토큰수)에 따른 리뷰의 수 살펴보기\n",
        "# 대부분의 리뷰가 500개 이하의 토큰만 사용했음을 알 수 있음\n",
        "plt.hist(num_tokens, bins=50)\n",
        "plt.xlabel('Number of tokens')\n",
        "plt.ylabel('Number of reviews')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BzLYeiVRBXc2",
        "outputId": "5d54998c-47c3-473c-914a-fc4284d5c7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwAj_aOIt2WV"
      },
      "source": [
        "The max number of tokens we will allow is set to the average plus 2 standard deviations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_M8wDkUJt2WV",
        "outputId": "0048f2d6-68f7-463f-f7c7-e2d6b80b46b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# TODO: num_tokens에서 평균 + 2 * 표준편차를 구하여 우리가 허용할 최대 단어의 개수인 max_tokens를 설정해보세요.\n",
        "# num_tokens가 정규분포를 따른다면 97% 이상이 max_tokens보다 적은 단어만을 사용할 것입니다.\n",
        "# Hint: np.mean과 np.std 사용\n",
        "\n",
        "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B34-DJ_ot2WV"
      },
      "source": [
        "This covers about 95% of the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Vg-lkIWjt2WV",
        "outputId": "cec5514a-f98f-4cab-c853-2bcfedced83d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.94532)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# max_tokens보다 짧은 리뷰의 비율을 출력하는 코드\n",
        "np.sum(num_tokens < max_tokens) / len(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "리뷰의 94% 이상은 max_tokens보다 짧으므로 공백을 삽입하여 길이를 max_tokens로 맞춰주고, max_tokens보다 긴 리뷰의 길이는 max_tokens로 자를 필요가 있습니다. 모든 리뷰를 가장 길었던 리뷰의 길이로 맞췄다면 데이터셋 크기가 굉장히 커졌을 것입니다.<br>\n",
        "\n",
        "공백을 삽입할 때 리뷰의 앞에 ('pre') 삽입할 수도 있고 마지막 ('post')에 삽입할 수도 있는데 인공신경망에 혼란을 주지 않기 위해 일관된 방식으로 삽입해야 합니다. (자르기도 동일)"
      ],
      "metadata": {
        "id": "cX2jx6eINEJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "M8Y_JY2St2WW"
      },
      "outputs": [],
      "source": [
        "# 공백을 리뷰의 앞에 삽입\n",
        "pad = 'pre'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XkY-yQDgt2WW"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터의 길이를 max_tokens로 맞춰주기\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,  # 입력의 최대 길이는 max_tokens\n",
        "                            padding=pad, truncating=pad)        # 패딩과 자르기 모두 리뷰의 앞부분에서 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "okwoNpgGt2WW"
      },
      "outputs": [],
      "source": [
        "# TODO: 테스트 데이터의 길이를 max_tokens로 맞춰주세요.\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
        "                           padding=pad, truncating=pad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCKTP6vPt2WW"
      },
      "source": [
        "We have now transformed the training-set into one big matrix of integers (tokens) with this shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LVp4YkW7t2WW",
        "outputId": "72e67b2d-dda6-4fd3-a3fe-3090e74e3603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 544)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# 훈련 데이터: (리뷰 개수, 각 리뷰의 길이)\n",
        "x_train_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrnD4H3Zt2WW"
      },
      "source": [
        "The matrix for the test-set has the same shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "scrolled": true,
        "id": "qJRJa6rgt2WW",
        "outputId": "5ecdc019-c110-4b75-db26-fd370fe4794a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 544)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# 테스트 데이터: (리뷰 개수, 각 리뷰의 길이)\n",
        "x_test_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxij8nRt2WW"
      },
      "source": [
        "For example, we had the following sequence of tokens above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LjIbxctIt2WX",
        "outputId": "1b5ac515-4c96-4177-9b8f-0010db7fdf77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2141, 1427,    2,  363,    4, 4161, 3922,   11,  491, 1694, 1703,\n",
              "       1029,    6,  831,    5,   22,   31, 1199,  164,    1,  128,  515,\n",
              "         47,    4,    1,  116,  104, 2860, 2249,  443,   45, 1033,    1,\n",
              "        310,  286,   23,  311,    2, 3233,    1,  111,    6,    3,  226,\n",
              "         20,    1, 6856,  486,   18,    6,    3, 2523,  665,   36,  104,\n",
              "         12, 1774,   65,  309,   14,   43,   33,   70, 6471,   43,  532,\n",
              "          6,   21,  125,  188,   11,  200,   21,   26,  125,   17,  273,\n",
              "         22,  142,  139,   63,    1, 2411,   36,    1,  369,   54, 1126,\n",
              "       1393,   18,   43,   22,   23,  256,   15,  137,   51,  237,  117,\n",
              "         22,   80,  165,    9])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# 패딩 전의 0번째 훈련 데이터\n",
        "np.array(x_train_tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVbARkK0t2WX"
      },
      "source": [
        "This has simply been padded to create the following sequence. Note that when this is input to the Recurrent Neural Network, then it first inputs a lot of zeros. If we had padded 'post' then it would input the integer-tokens first and then a lot of zeros. This may confuse the Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WvblmFmht2WX",
        "outputId": "1819ad54-67bb-4110-9e6a-60795d4cc8fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0 2141 1427    2  363    4 4161 3922\n",
            "   11  491 1694 1703 1029    6  831    5   22   31 1199  164    1  128\n",
            "  515   47    4    1  116  104 2860 2249  443   45 1033    1  310  286\n",
            "   23  311    2 3233    1  111    6    3  226   20    1 6856  486   18\n",
            "    6    3 2523  665   36  104   12 1774   65  309   14   43   33   70\n",
            " 6471   43  532    6   21  125  188   11  200   21   26  125   17  273\n",
            "   22  142  139   63    1 2411   36    1  369   54 1126 1393   18   43\n",
            "   22   23  256   15  137   51  237  117   22   80  165    9]\n"
          ]
        }
      ],
      "source": [
        "# TODO: 패딩 후의 0번째 훈련 데이터를 출력해보세요.\n",
        "# Hint: 바로 위와 다른 형태로 나와야 합니다.\n",
        "print(np.array(x_train_pad[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuEuwTb7t2WX"
      },
      "source": [
        "## Tokenizer Inverse Map\n",
        "\n",
        "정수의 배열을 다시 텍스트로 바꿔주는 함수를 만들어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "GcQFzRXkt2WX"
      },
      "outputs": [],
      "source": [
        "idx = tokenizer.word_index # tokenizer는 단어 (key)-> 정수 (value)이므로\n",
        "inverse_map = dict(zip(idx.values(), idx.keys())) # 정수 (value) -> 단어 (key)로 보내면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUsOFMx3t2WX"
      },
      "source": [
        "Helper-function for converting a list of tokens back to a string of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CmNXSZX3t2WX"
      },
      "outputs": [],
      "source": [
        "# tokens_to_string 함수에 정수 배열을 입력하면 텍스트로 출력됩니다.\n",
        "def tokens_to_string(tokens):\n",
        "    # Map from tokens back to words.\n",
        "    words = [inverse_map[token] for token in tokens if token != 0]\n",
        "\n",
        "    # Concatenate all words.\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8joKACPJt2WX"
      },
      "source": [
        "For example, this is the original text from the data-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "scrolled": true,
        "id": "G7-Rex8ot2WX",
        "outputId": "43721a72-9b58-4029-fb15-db9e1aa65f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Visually stunning and full of Eastern Philosophy, this amazing martial arts fantasy is brought to you by master director Tsui Hark, the man behind some of the best films Hong Kong cinema has produced. The special effects are beautiful and imaginative. The plot is a bit on the cerebral side, but is a refreshing change from films that treat their audience as if they were morons. If thinking is not your forte, however, this may not be your movie. Maybe you should go see the latest from the Hollywood studio's no brain club, but if you are looking for something more, he's where you will find it.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# 훈련 데이터의 0번째 리뷰 실제 텍스트\n",
        "x_train_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수로 변환된 훈련 데이터의 0번째 리뷰\n",
        "np.array(x_train_tokens[0])"
      ],
      "metadata": {
        "id": "1q-SfSwdQQS2",
        "outputId": "20278152-9c62-4257-d2d9-f58ce03b5526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2141, 1427,    2,  363,    4, 4161, 3922,   11,  491, 1694, 1703,\n",
              "       1029,    6,  831,    5,   22,   31, 1199,  164,    1,  128,  515,\n",
              "         47,    4,    1,  116,  104, 2860, 2249,  443,   45, 1033,    1,\n",
              "        310,  286,   23,  311,    2, 3233,    1,  111,    6,    3,  226,\n",
              "         20,    1, 6856,  486,   18,    6,    3, 2523,  665,   36,  104,\n",
              "         12, 1774,   65,  309,   14,   43,   33,   70, 6471,   43,  532,\n",
              "          6,   21,  125,  188,   11,  200,   21,   26,  125,   17,  273,\n",
              "         22,  142,  139,   63,    1, 2411,   36,    1,  369,   54, 1126,\n",
              "       1393,   18,   43,   22,   23,  256,   15,  137,   51,  237,  117,\n",
              "         22,   80,  165,    9])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya16yBQrt2WY"
      },
      "source": [
        "We can recreate this text except for punctuation and other symbols, by converting the list of tokens back to words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DqDwOJ9Lt2WY",
        "outputId": "0eeef1de-dfee-491f-ad3a-27698f7c5631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visually stunning and full of eastern philosophy this amazing martial arts fantasy is brought to you by master director the man behind some of the best films hong kong cinema has produced the special effects are beautiful and imaginative the plot is a bit on the cerebral side but is a refreshing change from films that treat their audience as if they were morons if thinking is not your however this may not be your movie maybe you should go see the latest from the hollywood no brain club but if you are looking for something more he's where you will find it\n"
          ]
        }
      ],
      "source": [
        "# TODO: 정수 형태의 0번째 리뷰를 다시 단어로 변환해보세요.\n",
        "# 모두 소문자로 변경되고, 잘 쓰이지 않는 단어들은 삭제된 상태입니다.\n",
        "# Hint: 위에서 정의한 tokens_to_string 함수 이용\n",
        "reconstructed_text = tokens_to_string(x_train_pad[0])\n",
        "print(reconstructed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hamblZDft2WY"
      },
      "source": [
        "## Create the Recurrent Neural Network\n",
        "\n",
        "We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial #03-C for a tutorial on Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8nObzn4ft2WY"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggam4ai4t2WY"
      },
      "source": [
        "자연어 처리를 위한 순환 신경망의 가장 첫 번째 층은 임베딩층입니다. 우리가 선택한 10000개의 단어를 우리가 지정한 차원에 보내 비슷한 단어가 가까운 곳에 위치하도록 만들 것입니다. (Tokenizer가 만든 정수는 1: the, 2: and, 3: a, 4: of과 같이 뜻이 비슷하지도 않은데 단어들이 가까운 곳에 위치해 있습니다.)\n",
        "\n",
        "일반적으로 임베딩 차원은 100에서 300 정도를 사용하는데 일단 8차원만 사용해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ObHSK_FKt2WY"
      },
      "outputs": [],
      "source": [
        "embedding_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnOGpWR8t2WY"
      },
      "source": [
        "The embedding-layer also needs to know the number of words in the vocabulary (`num_words`) and the length of the padded token-sequences (`max_tokens`). We also give this layer a name because we need to retrieve its weights further below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sVBLbYdrt2WY",
        "outputId": "39b68d9f-1873-4f41-c3d8-206d56fdc735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 임베딩층 추가\n",
        "model.add(Embedding(input_dim=num_words,      # 사용하는 단어의 개수\n",
        "                    output_dim=embedding_size,# 임베딩 차원\n",
        "                    input_length=max_tokens,  # 리뷰의 길이\n",
        "                    name='layer_embedding'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36FqWbR9t2WY"
      },
      "source": [
        "순환 유닛인 GRU를 추가하고 Dense로 묶어 하나의 출력에서 0과 1사이의 실수를 출력하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LnuPpp4zt2WY"
      },
      "outputs": [],
      "source": [
        "model.add(GRU(units=256))\n",
        "model.add(Dense(1, activation='sigmoid')) # 이전 층의 입력을 받아들여 0부터 1사이의 실수값 하나 출력 (sigmoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUlN1Vxvt2WZ"
      },
      "source": [
        "Use the Adam optimizer with the given learning-rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "h9tJRHtZt2WZ"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kah4uUD3t2WZ"
      },
      "source": [
        "Compile the Keras model so it is ready for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KRiczqCyt2WZ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ZINq_6Bkt2Wa",
        "outputId": "176fa0d4-652b-423d-d4c5-51046a7e5cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ layer_embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ layer_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZYcJVRpt2Wa"
      },
      "source": [
        "## Train the Recurrent Neural Network\n",
        "\n",
        "We can now train the model. Note that we are using the data-set with the padded sequences. We use 5% of the training-set as a small validation-set, so we have a rough idea whether the model is generalizing well or if it is perhaps over-fitting to the training-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "scrolled": true,
        "id": "tQxdKqGbt2Wa",
        "outputId": "4c0fd121-6ea7-444b-a11f-5cb0ba6ed6b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 38ms/step - accuracy: 0.6484 - loss: 0.6376 - val_accuracy: 0.8178 - val_loss: 0.4295\n",
            "Epoch 2/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 38ms/step - accuracy: 0.8609 - loss: 0.3315 - val_accuracy: 0.6528 - val_loss: 0.6087\n",
            "Epoch 3/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.8333 - loss: 0.3862 - val_accuracy: 0.7948 - val_loss: 0.5122\n",
            "Epoch 4/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.9236 - loss: 0.2034 - val_accuracy: 0.8998 - val_loss: 0.2811\n",
            "Epoch 5/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.9398 - loss: 0.1698 - val_accuracy: 0.8952 - val_loss: 0.2946\n",
            "Epoch 6/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.9553 - loss: 0.1357 - val_accuracy: 0.8582 - val_loss: 0.4304\n",
            "Epoch 7/10000\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.9627 - loss: 0.1176 - val_accuracy: 0.7740 - val_loss: 0.7186\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7956a8205ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# validation을 통해 테스트셋에서의 성능을 가늠할 수 있습니다.\n",
        "# 훈련데이터에 대한 성능은 개선되는데 validation에서 악화된다면 과적합이 일어나는 것일 수도 있습니다.\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# TODO: 자유롭게 설정, restore_best_weights=True 추천\n",
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   patience=3,\n",
        "                   restore_best_weights=True)\n",
        "\n",
        "model.fit(x_train_pad, y_train, callbacks=es,\n",
        "          validation_split=0.2, epochs=10000, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "382P8ZGMt2Wa"
      },
      "source": [
        "## Performance on Test-Set\n",
        "\n",
        "Now that the model has been trained we can calculate its classification accuracy on the test-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hP1eEs8xt2Wa",
        "outputId": "42aef8df-644c-4f7e-d8cc-d3a59ce72a1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8406 - loss: 0.3797\n",
            "CPU times: user 7.92 s, sys: 412 ms, total: 8.34 s\n",
            "Wall time: 10.3 s\n"
          ]
        }
      ],
      "source": [
        "# 테스트셋에서의 성능 평가\n",
        "%%time\n",
        "result = model.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alpR4ZXut2Wc"
      },
      "source": [
        "## New Data\n",
        "\n",
        "Let us try and classify new texts that we make up. Some of these are obvious, while others use negation and sarcasm to try and confuse the model into mis-classifying the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7dguFHY5t2Wc"
      },
      "outputs": [],
      "source": [
        "# 새로운 데이터셋 8개 생성하여 texts에 저장\n",
        "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
        "text2 = \"Good movie!\"\n",
        "text3 = \"Maybe I like this movie.\"\n",
        "text4 = \"Meh ...\"\n",
        "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
        "text6 = \"Bad movie!\"\n",
        "text7 = \"Not a good movie!\"\n",
        "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
        "texts = [text1, text2, text3, text4, text5, text6, text7, text8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L21c2SlFt2Wc"
      },
      "source": [
        "We first convert these texts to arrays of integer-tokens because that is needed by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "N53OFtJxt2Wd"
      },
      "outputs": [],
      "source": [
        "# TODO: tokenizer로 texts를 정수로 변환하세요.\n",
        "tokens = tokenizer.texts_to_sequences(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tX0cSgCt2Wd"
      },
      "source": [
        "To input texts with different lengths into the model, we also need to pad and truncate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "gCZytuX-t2Wd",
        "outputId": "b43c7d5c-b8d2-4b10-9520-57507eebf204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 544)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# TODO: 패딩으로 tokens의 길이를 맞춰주세요.\n",
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
        "tokens_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPwl4x3nt2Wd"
      },
      "source": [
        "We can now use the trained model to predict the sentiment for these texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5smpP0MOt2Wd",
        "outputId": "8ee1b3e7-928e-453f-9c18-bca21d40e25c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "Text 1 Prediction: 0.9878 → 긍정\n",
            "Text 2 Prediction: 0.9955 → 긍정\n",
            "Text 3 Prediction: 0.9012 → 긍정\n",
            "Text 4 Prediction: 1.0000 → 긍정\n",
            "Text 5 Prediction: 0.8510 → 긍정\n",
            "Text 6 Prediction: 0.8664 → 긍정\n",
            "Text 7 Prediction: 0.9780 → 긍정\n",
            "Text 8 Prediction: 0.1213 → 부정\n"
          ]
        }
      ],
      "source": [
        "# TODO: model로 tokens_pad를 예측해보세요.\n",
        "# 1에 가까울 수록 긍정적이고 0에 가까울 수록 부정적입니다.\n",
        "# 잘 예측했나요? 운에 따라 결과가 다를 수 있습니다.\n",
        "predictions = model.predict(tokens_pad)\n",
        "for i, p in enumerate(predictions):\n",
        "    print(f\"Text {i+1} Prediction: {p[0]:.4f} → {'긍정' if p[0] >= 0.5 else '부정'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYhK-DEot2Wd"
      },
      "source": [
        "A value close to 0.0 means a negative sentiment and a value close to 1.0 means a positive sentiment. These numbers will vary every time you train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Newer Data\n",
        "우리 모델은 평균 200개 이상의 단어가 쓰인 리뷰로 학습됐습니다. 위와 같이 매우 짧은 리뷰에 대해서는 잘 동작하지 않을 수도 있으니 <br>IMDb에서 듄 Part Two의 10점짜리 리뷰와 5점 이하의 리뷰로 테스트 해봅시다.<br>\n",
        "https://www.imdb.com/title/tt15239678/reviews"
      ],
      "metadata": {
        "id": "aEL_WlY7HzWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = ''' If you liked or loved the first one, the same will apply for this one. Personally, I loved this one even more and I think general audiences will as well. So I hope it does well at the box office because I need Dune Messiah now. This is everything I love about going to the movies. Also, Hans Zimmer.\n",
        "\n",
        "For book readers, I'd say this one takes more liberties than the first, but they were changes I liked, some for the better even. For the most part, it still hits the main beats from the book with a few things altered. The biggest change being no time jump. Therefore, a certain character doesn't fully appear in the movie contrary to the book. Though, they still use the character in a different way that I liked personally.\n",
        "\n",
        "I know it's early in the year, but you can lock some Oscar nominations for this movie: Best Picture, Best Director, Best Adapted Screenplay, Best Editing, Best Cinematography, Best Score, Best Sound, Best Visual Effects, Best Costume, Best Production Design, Best Makeup, and hopefully Rebecca Ferguson this time for Best Supporting Actress. I guess that's pretty much every category now that I've typed it out lol. '''\n",
        "text2 = ''' This is the kind of movie that is impossible to do justice, just by talking about it! It is the kind of experience you had once.. but you never thought you would get again.. until this movie proves you WRONG!!\n",
        "\n",
        "This movie takes the aspects of the first movie and improves upon them in almost every way possible, already writing itself into the books of greatest sequels of ALL TIME!!\n",
        "\n",
        "Everything in this movie was TOP notch! Dennis Villeneauve proves why he is not only a master of Sci-Fi movies, but just filmmaking in general! This was a demonstration of power, in what you can achieve with filmmaking!\n",
        "\n",
        "I beg you to watch this on the biggest and best screen you can find! This will be an experience that will stick with you forever!\n",
        "\n",
        "I thought the real Blockbuster was dead.. but I was wrong! This is the cinematic experience i craved!\n",
        "\n",
        "Now I am gonna watch this movie again, and maybe again! Then I am gonna read the books! '''\n",
        "text3 = ''' As a sci-fi enthusiast, I approached the latest Dune 2 movie with a sense of anticipation, hoping to be swept away by its grandeur and depth. However, what I encountered left me feeling underwhelmed and disconnected. The film seemed to lack the spark that ignites the imagination, opting instead for a plodding pace that failed to engage me fully.\n",
        "\n",
        "Perhaps it was my own weariness or a misalignment with the source material, but I struggled to find moments worth revisiting after the credits rolled. The supposed \"best scenes\" seemed to blur together in a haze of monotony, leaving me searching for a standout moment or two to cling to.\n",
        "\n",
        "While I can appreciate the efforts of the filmmakers and the ambition behind bringing such a beloved story to life, I can't help but feel that the overwhelming hype surrounding this adaptation is somewhat unwarranted. Despite its visual spectacle and star-studded cast, Dune 2 left me yearning for a deeper connection and a stronger emotional resonance that simply wasn't present.\n",
        "\n",
        "In the end, while the film may find its audience among die-hard fans of the franchise or those with a particular affinity for its themes, for me, it fell short of the lofty expectations set by its predecessors and failed to leave a lasting impression. '''\n",
        "text4 = ''' This second part of the movie sets the tone much in the same way as the first movie. Making the movie feels like cramming alot of story and backdrop in a way too small time frame. Which is logical considering Dune spans several novels with a history of 15.000 years of civilization. Villeneuve however put more focus on aesthetics rather than storytelling, Which makes the movie a bland and confusing thing to watch. The pacing is irregular, at some parts dragging alot and towards the end you get the feeling that it is rapping it up in a unsatisfying speed. Especially for people who are not familiair with the original material, you don't get the sense what the backstories are of the characters and why they made the decisions they made. Cramming a story like Dune in a two part sets of films, is probably not doing the story justice, should have been remade into an tv series to make it really work, therefore the rewatchability of this rendition of Dune is very unlikely. '''\n",
        "texts = [text1, text2, text3, text4]"
      ],
      "metadata": {
        "id": "_Jqjcb2sH0zG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: 상기 texts에 대해 예측해보세요.\n",
        "#Hint: tokenizer와 padding을 이용해야 합니다.\n",
        "tokens = tokenizer.texts_to_sequences(texts)\n",
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
        "\n",
        "# 예측\n",
        "predictions = model.predict(tokens_pad)\n",
        "for i, p in enumerate(predictions):\n",
        "    print(f\"리뷰 {i+1} 예측 점수: {p[0]:.4f} → {'긍정' if p[0] >= 0.5 else '부정'}\")"
      ],
      "metadata": {
        "id": "VN2wWdHDJJNS",
        "outputId": "a1a4a70c-f135-4016-ba0a-3078eabcafa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
            "리뷰 1 예측 점수: 0.9609 → 긍정\n",
            "리뷰 2 예측 점수: 0.7796 → 긍정\n",
            "리뷰 3 예측 점수: 0.1409 → 부정\n",
            "리뷰 4 예측 점수: 0.4552 → 부정\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THYTOZSet2Wd"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "임베딩을 통해 각 단어가 어떻게 변환되는지 살펴봅시다.\n",
        "\n",
        "The model cannot work on integer-tokens directly, because they are integer values that may range between 0 and the number of words in our vocabulary, e.g. 10000. So we need to convert the integer-tokens into vectors of values that are roughly between -1.0 and 1.0 which can be used as input to a neural network.\n",
        "\n",
        "This mapping from integer-tokens to real-valued vectors is also called an \"embedding\". It is essentially just a matrix where each row contains the vector-mapping of a single token. This means we can quickly lookup the mapping of each integer-token by simply using the token as an index into the matrix. The embeddings are learned along with the rest of the model during training.\n",
        "\n",
        "Ideally the embedding would learn a mapping where words that are similar in meaning also have similar embedding-values. Let us investigate if that has happened here.\n",
        "\n",
        "First we need to get the embedding-layer from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "jtyAVJbpt2Wd"
      },
      "outputs": [],
      "source": [
        "# 임베딩 층 불러오기\n",
        "layer_embedding = model.get_layer('layer_embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jANfbGKt2Wd"
      },
      "source": [
        "We can then get the weights used for the mapping done by the embedding-layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "uVO-XSXrt2We"
      },
      "outputs": [],
      "source": [
        "# 임베딩 층의 가중치 불러오기\n",
        "weights_embedding = layer_embedding.get_weights()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6qnaB0t2We"
      },
      "source": [
        "Note that the weights are actually just a matrix with the number of words in the vocabulary times the vector length for each embedding. That's because it is basically just a lookup-matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "-YcE3C_kt2We",
        "outputId": "3e0331f4-9fa9-49f8-ff0e-1b69dba6926c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# 10,000개의 단어를 8차원으로 보내고 있습니다.\n",
        "weights_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwqGhFLVt2We"
      },
      "source": [
        "Let us get the integer-token for the word 'good', which is just an index into the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "M9xEoo2Ht2We",
        "outputId": "99feba5e-bc66-4e54-d851-3009eb746c48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# tokenizer에서 good의 위치\n",
        "token_good = tokenizer.word_index['good']\n",
        "token_good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1XR9Gut2We"
      },
      "source": [
        "Let us also get the integer-token for the word 'great'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fFz5EWFPt2We",
        "outputId": "0290806b-3618-4595-929d-047f585cadee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# tokenizer에서 great의 위치\n",
        "token_great = tokenizer.word_index['great']\n",
        "token_great"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1itM760At2Wf"
      },
      "source": [
        "These integertokens may be far apart and will depend on the frequency of those words in the data-set.\n",
        "\n",
        "Now let us compare the vector-embeddings for the words 'good' and 'great'. Several of these values are similar, although some values are quite different. Note that these values will change every time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Ju_7MpFrt2Wf",
        "outputId": "af3d0a91-dcef-4db7-dac6-caf7db60007d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.07965372,  0.09393247, -0.07351585, -0.07625568,  0.03808469,\n",
              "       -0.02028265,  0.04475041, -0.07350552], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# good의 임베딩 결과\n",
        "weights_embedding[token_good]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "K4aw3__At2Wf",
        "outputId": "bd09a095-f45f-4e6f-b98c-465d53a2aecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.17436583,  0.06022424, -0.14172828, -0.14446627, -0.15917946,\n",
              "       -0.12657806,  0.08399598, -0.12932244], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# great의 임베딩 결과\n",
        "weights_embedding[token_great]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrbHzmFlt2Wf"
      },
      "source": [
        "Similarly, we can compare the embeddings for the words 'bad' and 'horrible'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "WYOkp0DIt2Wf"
      },
      "outputs": [],
      "source": [
        "# tokenizer에서 bad와 horrible\n",
        "token_bad = tokenizer.word_index['bad']\n",
        "token_horrible = tokenizer.word_index['horrible']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DhfbcEnit2Wf",
        "outputId": "56e33df7-d03e-444d-b446-04d0fb0fa407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'bad': [-0.08484949 -0.09777339  0.15375751  0.13102603  0.08843281  0.10991532\n",
            " -0.06374491  0.09810369]\n"
          ]
        }
      ],
      "source": [
        "# TODO: bad의 임베딩 결과\n",
        "print(\"Embedding for 'bad':\", weights_embedding[token_bad])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "8uqa8Krzt2Wf",
        "outputId": "a16159f6-6371-4912-f474-90edaaa757d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'horrible': [-0.14858234 -0.17827055  0.1267179   0.12521534  0.14907832  0.1455547\n",
            " -0.16841762  0.19272292]\n"
          ]
        }
      ],
      "source": [
        "# TODO: horrible의 임베딩 결과\n",
        "print(\"Embedding for 'horrible':\", weights_embedding[token_horrible])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩 결과를 보면 good과 great은 비슷한 곳에 위치하고 good과 bad는 반대되는 곳에 위치한 것을 볼 수 있습니다."
      ],
      "metadata": {
        "id": "0zOsin_OXnsU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gDSIg26t2Wf"
      },
      "source": [
        "### Sorted Words\n",
        "\n",
        "코사인 유사도로 임베딩 공간에서 비슷한 단어를 찾아봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "YyhM3RL0t2Wf"
      },
      "outputs": [],
      "source": [
        "# 주어진 단어와 비슷한 단어를 찾아주는 함수\n",
        "def print_sorted_words(word, metric='cosine'):\n",
        "    \"\"\"\n",
        "    Print the words in the vocabulary sorted according to their\n",
        "    embedding-distance to the given word.\n",
        "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the token (i.e. integer ID) for the given word.\n",
        "    token = tokenizer.word_index[word]\n",
        "\n",
        "    # Get the embedding for the given word. Note that the\n",
        "    # embedding-weight-matrix is indexed by the word-tokens\n",
        "    # which are integer IDs.\n",
        "    embedding = weights_embedding[token]\n",
        "\n",
        "    # Calculate the distance between the embeddings for\n",
        "    # this word and all other words in the vocabulary.\n",
        "    distances = cdist(weights_embedding, [embedding],\n",
        "                      metric=metric).T[0]\n",
        "\n",
        "    # Get an index sorted according to the embedding-distances.\n",
        "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
        "    sorted_index = np.argsort(distances)\n",
        "\n",
        "    # Sort the embedding-distances.\n",
        "    sorted_distances = distances[sorted_index]\n",
        "\n",
        "    # Sort all the words in the vocabulary according to their\n",
        "    # embedding-distance. This is a bit excessive because we\n",
        "    # will only print the top and bottom words.\n",
        "    sorted_words = [inverse_map[token] for token in sorted_index\n",
        "                    if token != 0]\n",
        "\n",
        "    # Helper-function for printing words and embedding-distances.\n",
        "    def _print_words(words, distances):\n",
        "        for word, distance in zip(words, distances):\n",
        "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
        "\n",
        "    # Number of words to print from the top and bottom of the list.\n",
        "    k = 10\n",
        "\n",
        "    print(\"Distance from '{0}':\".format(word))\n",
        "\n",
        "    # Print the words with smallest embedding-distance.\n",
        "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
        "\n",
        "    print(\"...\")\n",
        "\n",
        "    # Print the words with highest embedding-distance.\n",
        "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewbOIavTt2Wg"
      },
      "source": [
        "We can then print the words that are near and far from the word 'great' in terms of their vector-embeddings. Note that these may change each time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "scrolled": true,
        "id": "CFU1vStzt2Wg",
        "outputId": "27b81b67-12f0-4f35-c8ee-0953ae8dade6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance from 'great':\n",
            "0.000 - great\n",
            "0.006 - reviewers\n",
            "0.008 - center\n",
            "0.009 - keeper\n",
            "0.011 - underwater\n",
            "0.011 - vibrant\n",
            "0.012 - episodes\n",
            "0.013 - josie\n",
            "0.015 - rap\n",
            "0.015 - rare\n",
            "...\n",
            "1.982 - skipping\n",
            "1.983 - cheated\n",
            "1.983 - radiation\n",
            "1.983 - trite\n",
            "1.983 - baldwin\n",
            "1.985 - dry\n",
            "1.985 - ugh\n",
            "1.985 - favor\n",
            "1.985 - bury\n",
            "1.990 - tolerable\n"
          ]
        }
      ],
      "source": [
        "# great와 비슷한 단어 10개, 반대 단어 10개\n",
        "print_sorted_words('great', metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZi34Trt2Wg"
      },
      "source": [
        "Similarly, we can print the words that are near and far from the word 'worst' in terms of their vector-embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "scrolled": true,
        "id": "P3qmBJ95t2Wg",
        "outputId": "58f9ce11-9e2c-46b3-b0b7-7b1735fd9e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance from 'worst':\n",
            "0.000 - worst\n",
            "0.004 - damme\n",
            "0.004 - uninspired\n",
            "0.006 - predictable\n",
            "0.006 - valley\n",
            "0.006 - corridors\n",
            "0.006 - boring\n",
            "0.007 - unoriginal\n",
            "0.007 - jigsaw\n",
            "0.008 - unwatchable\n",
            "...\n",
            "1.995 - greatly\n",
            "1.995 - fascinating\n",
            "1.995 - jacob\n",
            "1.995 - lucy\n",
            "1.995 - testament\n",
            "1.995 - villains\n",
            "1.996 - complaints\n",
            "1.996 - definitely\n",
            "1.997 - avoids\n",
            "1.998 - perfect\n"
          ]
        }
      ],
      "source": [
        "# TODO: worst와 비슷한 단어 10개, 반대 단어 10개\n",
        "print_sorted_words('worst', metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdkL2Jubt2Wg"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "감성 분석에서 순환 신경망은 비교적 만족스러운 성능을 보이지만 사람과는 전혀 다른 방식으로 감정을 계산해냅니다. 거대 언어 모델에 비하여 상당히 적은 자원만을 사용했기 때문에 성능이 좋게 나오지 않았으니 참고하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnclb77Ct2Wg"
      },
      "source": [
        "## 연습문제 (수행할 필요 없습니다.)\n",
        "\n",
        "수행할 때마다 다른 결과를 얻을 수 있습니다.\n",
        "\n",
        "* 현재 Tokenizer가 가장 자주 쓰이는 10,000개의 단어를 처리하는데 5000개만 사용하면 성능이 어떻게 변할까요?\n",
        "* 임베딩을 8차원에서 수행하는데 200차원으로 늘리면 성능이 어떻게 변할까요?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTjDZ271t2Wg"
      },
      "source": [
        "## License (MIT)\n",
        "\n",
        "Copyright (c) 2022 by uramoon@kw.ac.kr<br>\n",
        "Copyright (c) 2018 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZdkL2Jubt2Wg",
        "wnclb77Ct2Wg",
        "NTjDZ271t2Wg"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}